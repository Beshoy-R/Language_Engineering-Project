{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNzItF4PQktu7uPU3BtTDoz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beshoy-R/Beshoy-R/blob/main/PROJECT_LanguageEngineering(1).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmhKqoBhFf96"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('dataset.csv')"
      ],
      "metadata": {
        "id": "DJK0dcwcWY1h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = dataset.columns\n",
        "print(columns)"
      ],
      "metadata": {
        "id": "oG0hnnQqLPeW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Dropping unwanted columns:**"
      ],
      "metadata": {
        "id": "Gosp8qiZQSbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(columns=['id', 'severe_toxic', 'obscene','obscene','threat','insult','identity_hate'])\n"
      ],
      "metadata": {
        "id": "5Imk_xldL-V0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "id": "kpxqaskIMPKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Cleaning the data(Tokenize,Removing Stopwords,Lemmatize):**\n"
      ],
      "metadata": {
        "id": "OKAEPzzLPvU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "stop_words = set(stopwords.words('english')).difference({\n",
        "    'against', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", \"didn't\", 'didn', 'doesn', \n",
        "    \"doesn't\", 'doing', 'don', \"don't\", 'down', 'did', 'can', 'had', 'hadn', \"hadn't\", \n",
        "    'has', 'hasn', \"hasn't\", 'have', 'haven', \"haven't\", 'is', 'isn', \"isn't\", 'mightn', \n",
        "    \"mightn't\", 'more', 'most', 'mustn', \"mustn't\", 'needn', \"needn't\", 'no', 'nor', \n",
        "    'not', 'off', 'on', 'should', \"should've\", 'shouldn', \"shouldn't\", 'so', 't', 'too', \n",
        "    'very', 'was', 'wasn', \"wasn't\", 'were', 'weren', \"weren't\", \"won't\", 'wouldn', \n",
        "    \"wouldn't\",\n",
        "})\n",
        "\n",
        "def preprocess_text(text):\n",
        "    # Remove URLs and mentions\n",
        "    text = re.sub(r'http\\S+|www.\\S+|@\\S+', '', text)\n",
        "    # Remove non-alphabetic characters and convert to lowercase\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())\n",
        "    # Tokenize the text\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "    # Lemmatize the words\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    # Join tokens back into a single string\n",
        "    text = ' '.join(tokens)\n",
        "    return text\n",
        "def nltk_tag_to_wordnet_tag(nltk_tag):\n",
        "    if nltk_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif nltk_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif nltk_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif nltk_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:          \n",
        "        return None\n",
        "        \n",
        "dataset['comment_text'] = dataset['comment_text'].apply(preprocess_text)\n"
      ],
      "metadata": {
        "id": "u6mJsSTppWD7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**###### Done Prepareing the data(Cleaning and Preprocessing) ######**"
      ],
      "metadata": {
        "id": "g2KG7kKlXk5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (1) Logistic Regression: ##**"
      ],
      "metadata": {
        "id": "uheIgo7-Tv57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "x0cVgtSaRIJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "LR_Model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "LR_Model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = LR_Model.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "LR_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Logistic_Regression_Accuracy: {LR_accuracy*100}%\")"
      ],
      "metadata": {
        "id": "IianBaATY3Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (2) Support Vector Machine: ##**"
      ],
      "metadata": {
        "id": "xjgD2xjxbZLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "MWxKcKRSbjO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize the Support Vector Machine model\n",
        "SVC_model = SVC()\n",
        "\n",
        "# Train the model on the training set\n",
        "SVC_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = SVC_model.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Support_Vector_Machine_Accuracy: {accuracy*100}%\")"
      ],
      "metadata": {
        "id": "EgJ_pnxfb2bI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (3) Naïve Bayes: ##**"
      ],
      "metadata": {
        "id": "F9LNcaGXdDFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "ZLfn9-9bh2uC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize the Naïve Bayes model (MultinomialNB)\n",
        "NB_model = MultinomialNB()\n",
        "\n",
        "# Train the model on the training set\n",
        "NB_model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = NB_model.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Naïve_Bayes_Accuracy: {accuracy*100}%\")"
      ],
      "metadata": {
        "id": "gETfQTLsh83u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (4) Artificial Neural Networks: ##**"
      ],
      "metadata": {
        "id": "k1Ek_ivzi3L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "YA9_6KRejunw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "# Initialize the Artificial Neural Network model\n",
        "ANN_model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "ANN_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training set\n",
        "ANN_model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_prob = ANN_model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Artificial_Neural_Networks_Accuracy: {accuracy*100}%\")"
      ],
      "metadata": {
        "id": "C9juA_-0jwXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "# Define the labels for the matrix\n",
        "labels = ['Non-Toxic', 'Toxic']\n",
        "\n",
        "# Plot the confusion matrix\n",
        "sns.heatmap(cm, annot=True, cmap='Blues', fmt='d', xticklabels=labels, yticklabels=labels)\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('True')\n",
        "plt.title('Confusion Matrix')\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "6raQ8JvL3RpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a list of toxic comments\n",
        "toxic_comments = dataset[dataset['toxic'] == 1]['comment_text']\n",
        "\n",
        "# Combine all toxic comments into a single string\n",
        "toxic_text = ' '.join(toxic_comments)\n",
        "\n",
        "# Tokenize the text into individual words\n",
        "tokens = nltk.word_tokenize(toxic_text)\n",
        "\n",
        "# Count the frequency of each word\n",
        "word_frequency = Counter(tokens)\n",
        "\n",
        "# Select the top N words and their frequencies\n",
        "top_words = word_frequency.most_common(20)\n",
        "words, frequencies = zip(*top_words)\n",
        "\n",
        "# Create a bar chart\n",
        "plt.bar(words, frequencies)\n",
        "\n",
        "# Set plot labels and title\n",
        "plt.xlabel('Words')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Toxic Comment Word Frequency')\n",
        "\n",
        "# Rotate x-axis labels for better readability\n",
        "plt.xticks(rotation=45)\n",
        "\n",
        "# Display the plot\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "nQ_y77gG5tTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TEST OUR COMMENT**"
      ],
      "metadata": {
        "id": "BPS3NUQ1rdqa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_comment = \"Have a happy day\"\n",
        "preprocessed_comment = preprocess_text(our_comment)\n",
        "vectorized_comment = vectorizer.transform([preprocessed_comment]).toarray()\n",
        "\n",
        "ANN_prediction_prob = ANN_model.predict(vectorized_comment)\n",
        "ANN_prediction = (ANN_prediction_prob > 0.5).astype(int)\n",
        "prediction_label_ANN = 'toxic' if ANN_prediction == 1 else 'non-toxic'\n",
        "print('ANN_Prediction:', prediction_label_ANN)\n",
        "\n",
        "NB_prediction_prob = NB_model.predict(vectorized_comment)\n",
        "NB_prediction = (NB_prediction_prob > 0.5).astype(int)\n",
        "prediction_label_NB = 'toxic' if NB_prediction == 1 else 'non-toxic'\n",
        "print('NB_Prediction:', prediction_label_NB)\n",
        "\n",
        "SVC_prediction_prob = SVC_model.predict(vectorized_comment)\n",
        "SVC_prediction = (SVC_prediction_prob > 0.5).astype(int)\n",
        "prediction_label_SVC = 'toxic' if SVC_prediction == 1 else 'non-toxic'\n",
        "print('SVC_Prediction:', prediction_label_SVC)\n",
        "\n",
        "LR_prediction_prob = LR_Model.predict(vectorized_comment)\n",
        "LR_prediction = (LR_prediction_prob > 0.5).astype(int)\n",
        "prediction_label_LR = 'toxic' if LR_prediction == 1 else 'non-toxic'\n",
        "print('LR_Prediction:', prediction_label_LR)\n"
      ],
      "metadata": {
        "id": "cSOyift2rc5n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**IMPROVE OUR TESTING**"
      ],
      "metadata": {
        "id": "lmvKDSzWTxaq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "our_comment = \"i hate you\"\n",
        "preprocessed_comment = preprocess_text(our_comment)\n",
        "vectorized_comment = vectorizer.transform([preprocessed_comment]).toarray()\n",
        "\n",
        "models = {\n",
        "    'ANN': (ANN_model, 'Artificial_Neural_Network'),\n",
        "    'NB': (NB_model, 'Naive_Bayes'),\n",
        "    'SVC': (SVC_model, 'Support_Vector_Machine'),\n",
        "    'LR': (LR_Model, 'Logistic_Regression')\n",
        "}\n",
        "\n",
        "for model_key, (model, model_label) in models.items():\n",
        "    prediction_prob = model.predict(vectorized_comment)\n",
        "    prediction = (prediction_prob > 0.5).astype(int)\n",
        "    prediction_label = 'toxic' if prediction == 1 else 'non-toxic'\n",
        "    print(f'{model_label} Prediction:', prediction_label)\n",
        "\n"
      ],
      "metadata": {
        "id": "cFV6ywo7TzIi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}