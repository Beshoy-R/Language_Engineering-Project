{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOang7x7ZZwFcWaEtYPw+NW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Beshoy-R/Beshoy-R/blob/main/LanguageEngineering(1)_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "cmhKqoBhFf96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b71026e-f6fc-4414-f033-ac05b66adce8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = pd.read_csv('train.csv')"
      ],
      "metadata": {
        "id": "DJK0dcwcWY1h"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = dataset.columns\n",
        "print(columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oG0hnnQqLPeW",
        "outputId": "ab22a4b7-8f6e-4cdc-c523-a0031a632634"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',\n",
            "       'insult', 'identity_hate'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Dropping unwanted columns:**"
      ],
      "metadata": {
        "id": "Gosp8qiZQSbe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = dataset.drop(columns=['id', 'severe_toxic', 'obscene','obscene','threat','insult','identity_hate'])\n"
      ],
      "metadata": {
        "id": "5Imk_xldL-V0"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kpxqaskIMPKX",
        "outputId": "54532d87-06f2-4df2-b849-7cdf5c4757c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           comment_text  toxic\n",
            "0     Explanation\\nWhy the edits made under my usern...      0\n",
            "1     D'aww! He matches this background colour I'm s...      0\n",
            "2     Hey man, I'm really not trying to edit war. It...      0\n",
            "3     \"\\nMore\\nI can't make any real suggestions on ...      0\n",
            "4     You, sir, are my hero. Any chance you remember...      0\n",
            "...                                                 ...    ...\n",
            "2997  Re: All Items\\nI know that you said I did some...      0\n",
            "2998  \"\\nSo you not going tell me why you created so...      0\n",
            "2999  John Phillip Key (born 9 August 1961, in Auckl...      0\n",
            "3000  The Billy the Kid article with my contribution...      0\n",
            "3001  Vandalizing the Macedonian towns \\n\\nFreestype...      0\n",
            "\n",
            "[3002 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Clean the text data by removing unwanted characters, converting to lowercase, and removing URLs or mentions:**\n"
      ],
      "metadata": {
        "id": "OKAEPzzLPvU0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    text = re.sub(r'http\\S+|www.\\S+|@\\S+', '', text)  # remove URLs and mentions\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text.lower())  # remove non-alphabetic characters and convert to lowercase\n",
        "    return text\n"
      ],
      "metadata": {
        "id": "tpgtwSXIPag3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['comment_text'] = dataset['comment_text'].apply(clean_text)"
      ],
      "metadata": {
        "id": "YstatW4ZPjix"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Tokenize words:**"
      ],
      "metadata": {
        "id": "xZJgULYnUzCH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_text(text):\n",
        "    return nltk.word_tokenize(text)\n",
        "\n",
        "dataset['comment_text'] = dataset['comment_text'].apply(tokenize_text)\n"
      ],
      "metadata": {
        "id": "1_ntGEZoUynw"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Removing Stop Words:**"
      ],
      "metadata": {
        "id": "TjmgyOIYQ_Hk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords.words('english'))\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "    return [word for word in tokens if word not in stopwords]\n",
        "\n",
        "dataset['comment_text'] = dataset['comment_text'].apply(remove_stopwords)\n"
      ],
      "metadata": {
        "id": "NI1QqFgIQBNB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Lemmatize the words to reduce them to their base form:**"
      ],
      "metadata": {
        "id": "_o5hAQULWp3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_words(tokens):\n",
        "    return [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "dataset['comment_text'] = dataset['comment_text'].apply(lemmatize_words)\n"
      ],
      "metadata": {
        "id": "bKBxVyNqWo_Y"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**-> Join the tokens back into a single string:**"
      ],
      "metadata": {
        "id": "ssVdN1OgXQ92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def join_tokens(tokens):\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "dataset['comment_text'] = dataset['comment_text'].apply(join_tokens)"
      ],
      "metadata": {
        "id": "GQWq6Do_XRXR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**###### Done Prepareing the data(Cleaning and Preprocessing) #########################################################**"
      ],
      "metadata": {
        "id": "g2KG7kKlXk5-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (1) Logistic Regression: ##**"
      ],
      "metadata": {
        "id": "uheIgo7-Tv57"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "x0cVgtSaRIJC"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize the Logistic Regression model\n",
        "LR_Model = LogisticRegression()\n",
        "\n",
        "# Train the model on the training set\n",
        "LR_Model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = LR_Model.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "LR_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Logistic_Regression_Accuracy: {LR_accuracy*100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IianBaATY3Zc",
        "outputId": "3356d3ff-5a3c-419d-9ebc-3fcb1b88e8c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Logistic_Regression_Accuracy: 90.34941763727122%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (2) Support Vector Machine: ##**"
      ],
      "metadata": {
        "id": "xjgD2xjxbZLy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "MWxKcKRSbjO_"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize the Support Vector Machine model\n",
        "model = SVC()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Support_Vector_Machine_Accuracy: {accuracy*100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EgJ_pnxfb2bI",
        "outputId": "cd8db3e1-bf3f-499d-a659-4e21b108ac2e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Support_Vector_Machine_Accuracy: 90.68219633943427%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (3) Na誰ve Bayes: ##**"
      ],
      "metadata": {
        "id": "F9LNcaGXdDFW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "ZLfn9-9bh2uC"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train)\n",
        "X_test = vectorizer.transform(X_test)\n",
        "\n",
        "# Initialize the Na誰ve Bayes model (MultinomialNB)\n",
        "model = MultinomialNB()\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Na誰ve_Bayes_Accuracy: {accuracy*100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gETfQTLsh83u",
        "outputId": "420a0e12-e5d6-4d55-84a0-477d2ec9c68a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Na誰ve_Bayes_Accuracy: 89.18469217970049%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**## ALGORITHM (4) Artificial Neural Networks: ##**"
      ],
      "metadata": {
        "id": "k1Ek_ivzi3L2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer"
      ],
      "metadata": {
        "id": "YA9_6KRejunw"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target variable (y)\n",
        "X = dataset['comment_text']\n",
        "y = dataset['toxic']\n",
        "\n",
        "# Split the data into training and testing sets (80% for training, 20% for testing)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize the TF-IDF vectorizer\n",
        "vectorizer = TfidfVectorizer()\n",
        "\n",
        "# Convert the text data to TF-IDF features\n",
        "X_train = vectorizer.fit_transform(X_train).toarray()\n",
        "X_test = vectorizer.transform(X_test).toarray()\n",
        "\n",
        "# Initialize the Artificial Neural Network model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train the model on the training set\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32, verbose=1)\n",
        "\n",
        "# Make predictions on the testing set\n",
        "y_pred_prob = model.predict(X_test)\n",
        "y_pred = (y_pred_prob > 0.5).astype(int)\n",
        "\n",
        "# Compute the accuracy of the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print the accuracy\n",
        "print(f\"Artificial_Neural_Networks_Accuracy: {accuracy*100}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9juA_-0jwXy",
        "outputId": "cc797fa7-ed97-4519-e3c3-03ae136956f8"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "76/76 [==============================] - 3s 27ms/step - loss: 0.5009 - accuracy: 0.8946\n",
            "Epoch 2/10\n",
            "76/76 [==============================] - 3s 36ms/step - loss: 0.2728 - accuracy: 0.9005\n",
            "Epoch 3/10\n",
            "76/76 [==============================] - 2s 30ms/step - loss: 0.2017 - accuracy: 0.9167\n",
            "Epoch 4/10\n",
            "76/76 [==============================] - 2s 27ms/step - loss: 0.1326 - accuracy: 0.9517\n",
            "Epoch 5/10\n",
            "76/76 [==============================] - 2s 27ms/step - loss: 0.0796 - accuracy: 0.9854\n",
            "Epoch 6/10\n",
            "76/76 [==============================] - 2s 24ms/step - loss: 0.0492 - accuracy: 0.9950\n",
            "Epoch 7/10\n",
            "76/76 [==============================] - 2s 23ms/step - loss: 0.0322 - accuracy: 0.9979\n",
            "Epoch 8/10\n",
            "76/76 [==============================] - 3s 33ms/step - loss: 0.0223 - accuracy: 0.9992\n",
            "Epoch 9/10\n",
            "76/76 [==============================] - 2s 25ms/step - loss: 0.0162 - accuracy: 0.9996\n",
            "Epoch 10/10\n",
            "76/76 [==============================] - 1s 15ms/step - loss: 0.0122 - accuracy: 0.9996\n",
            "19/19 [==============================] - 0s 3ms/step\n",
            "Artificial_Neural_Networks_Accuracy: 92.17970049916805%\n"
          ]
        }
      ]
    }
  ]
}